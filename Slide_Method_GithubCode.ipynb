{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HG7jsefa7Ytv",
        "q2-mqQHxjbp7",
        "LKtNtcAvWYHR",
        "NsO4J-H-x91p",
        "kx6ifdU4x5rc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swfzlsIOjG57",
        "outputId": "c3aa006a-d1e6-4a57-9160-282ac5cf31b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeTCajbg-Dfp"
      },
      "source": [
        "## install package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6_SE92PD-Dfp"
      },
      "outputs": [],
      "source": [
        "!pip install pydub SpeechRecognition\n",
        "!pip install requests\n",
        "!pip install deepgram-sdk\n",
        "\n",
        "!pip install pydub moviepy librosa\n",
        "!pip install SpeechRecognition\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7cMFpi0-Dfp"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import speech_recognition as sr\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, ImageClip, concatenate_videoclips\n",
        "from pydub import AudioSegment, effects\n",
        "from pydub import silence\n",
        "from pydub.silence import split_on_silence\n",
        "from pydub.silence import detect_silence\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "\n",
        "import json\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import textwrap\n",
        "from deepgram import DeepgramClient, PrerecordedOptions, FileSource\n",
        "\n",
        "import openai\n",
        "from skimage.metrics import structural_similarity as ssim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function"
      ],
      "metadata": {
        "id": "_kzZerUh-Dfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Video Frame Recognition"
      ],
      "metadata": {
        "id": "W7ECxxK0-Dfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_slide_changes(video_path, slides_detect_threshold):\n",
        "\n",
        "    THRESHOLD = slides_detect_threshold\n",
        "\n",
        "    clip = VideoFileClip(video_path)\n",
        "    fps = clip.fps\n",
        "    total_time = clip.duration\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    previous_frame = None\n",
        "    original_timestamps = []\n",
        "    original_timestamps.append(0)\n",
        "    slide_pictures = []\n",
        "\n",
        "\n",
        "    first_second_found = False\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if not first_second_found and frame_count / fps < 1:\n",
        "            resized_frame = cv2.resize(frame, (video_width, video_height))\n",
        "            slide_pictures.append(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))\n",
        "            first_second_found = True\n",
        "\n",
        "        if previous_frame is not None:\n",
        "            diff = cv2.absdiff(previous_frame, gray_frame)\n",
        "            difference = np.mean(diff)\n",
        "\n",
        "            if difference > THRESHOLD:\n",
        "                time_position = frame_count / fps\n",
        "                original_timestamps.append(time_position)\n",
        "\n",
        "                resized_frame = cv2.resize(frame, (video_width, video_height))\n",
        "                slide_pictures.append(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        previous_frame = gray_frame\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    original_timestamps.append(total_time)\n",
        "\n",
        "    return original_timestamps, slide_pictures"
      ],
      "metadata": {
        "id": "ksCL1JQb-Dfq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_repeat_pictures(detect_slide_pictures, original_timestamps):\n",
        "    results = []\n",
        "    delete_index = []\n",
        "\n",
        "    for i in range(len(detect_slide_pictures) - 1):\n",
        "        ssim_score = compute_ssim(detect_slide_pictures[i], detect_slide_pictures[i + 1])\n",
        "\n",
        "        if ssim_score > 0.99:\n",
        "            delete_index.append(i + 1)\n",
        "\n",
        "    delete_index = sorted(set(delete_index), reverse=True)\n",
        "    for index in delete_index:\n",
        "        del detect_slide_pictures[index]\n",
        "        del original_timestamps[index]\n",
        "\n",
        "    return detect_slide_pictures, original_timestamps"
      ],
      "metadata": {
        "id": "fn0NkIIz-Dfq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Time_group(original_timestamps, slide_pictures):\n",
        "    time_group = {}\n",
        "    index = 0\n",
        "    time_group[index] = [(original_timestamps[0], slide_pictures[0])]\n",
        "\n",
        "    for i in range(1, len(original_timestamps)-1):\n",
        "        if original_timestamps[i] - original_timestamps[i - 1] < 2:\n",
        "          time_group[index].append((original_timestamps[i], slide_pictures[i]))\n",
        "        else:\n",
        "          index += 1\n",
        "          time_group[index] = [(original_timestamps[i], slide_pictures[i])]\n",
        "\n",
        "    return time_group"
      ],
      "metadata": {
        "id": "ahNhV_qV-Dfq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ssim(img1, img2, threshold = 0.9):\n",
        "    gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
        "    gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
        "    score, _ = ssim(gray1, gray2, full=True)\n",
        "    return score\n",
        "\n",
        "def SSIM_group(time_group, ssim_threshold):\n",
        "\n",
        "    ssim_group = {}\n",
        "    new_index = 0\n",
        "\n",
        "    for key, images in time_group.items():\n",
        "        timestamps = [ts for ts, _ in images]\n",
        "        image_arrays = [img for _, img in images]\n",
        "        split_indices = []\n",
        "\n",
        "        for i in range(len(image_arrays) - 1):\n",
        "            ssim_score = compute_ssim(image_arrays[i], image_arrays[i + 1])\n",
        "\n",
        "            if ssim_score < ssim_threshold:\n",
        "                split_indices.append(i + 1)\n",
        "\n",
        "        if not split_indices:\n",
        "            ssim_group[new_index] = images\n",
        "            new_index += 1\n",
        "        else:\n",
        "\n",
        "            start = 0\n",
        "            for split_index in split_indices:\n",
        "                ssim_group[new_index] = images[start:split_index]\n",
        "                new_index += 1\n",
        "                start = split_index\n",
        "\n",
        "            if start < len(images):\n",
        "                ssim_group[new_index] = images[start:]\n",
        "                new_index += 1\n",
        "    return ssim_group"
      ],
      "metadata": {
        "id": "OIl4w_LD-Dfq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def silent_segment(ssim_group):\n",
        "    silent_segments = {}\n",
        "    original_text_timestamps = []\n",
        "    slide_pictures = []\n",
        "    index = 0\n",
        "\n",
        "    for index in range(len(ssim_group) - 1):\n",
        "        current_group = ssim_group[index]\n",
        "        next_group = ssim_group[index + 1]\n",
        "\n",
        "        duration = next_group[0][0] - current_group[0][0]\n",
        "\n",
        "        if duration <= 2:\n",
        "\n",
        "            if index - 1 in silent_segments:\n",
        "                silent_segments[index - 1][1] = next_group[0][0]\n",
        "            else:\n",
        "                silent_segments[index - 1] = [current_group[0][0], next_group[0][0]]\n",
        "        else:\n",
        "            original_text_timestamps.append(current_group[0][0])\n",
        "            slide_pictures.append(current_group[-1][1])\n",
        "\n",
        "    last_key = max(ssim_group.keys())\n",
        "    last_group = ssim_group[last_key]\n",
        "\n",
        "    if len(ssim_group) < 2 or (last_group[0][0] - ssim_group[last_key - 1][0][0]) > 2:\n",
        "        original_text_timestamps.append(last_group[0][0])\n",
        "        slide_pictures.append(last_group[-1][1])\n",
        "\n",
        "    if original_timestamps[-1] not in original_text_timestamps:\n",
        "        original_text_timestamps.append(original_timestamps[-1])\n",
        "\n",
        "    return silent_segments, original_text_timestamps, slide_pictures\n"
      ],
      "metadata": {
        "id": "WVjw4cni-Dfq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Audio Segmentation"
      ],
      "metadata": {
        "id": "8smm7VTB-Dfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_segments(video_path, timestamps, audioSegmentation):\n",
        "\n",
        "    audio_files = []\n",
        "    durations = []\n",
        "\n",
        "    video_clip = VideoFileClip(video_path)\n",
        "\n",
        "    for i in range(len(timestamps) - 1):\n",
        "        start_time = timestamps[i]\n",
        "        end_time = timestamps[i + 1]\n",
        "\n",
        "        audio_segment = video_clip.audio.subclip(start_time, end_time)\n",
        "        output_file = f\"{audioSegmentation}/original_audio_segment_{i}.wav\"\n",
        "\n",
        "        audio_segment.write_audiofile(output_file, codec='pcm_s16le')\n",
        "        audio_files.append(output_file)\n",
        "\n",
        "        durations.append(audio_segment.duration)\n",
        "\n",
        "    video_clip.close()\n",
        "\n",
        "    return audio_files, durations\n",
        ""
      ],
      "metadata": {
        "id": "v7_UrNtT-Dfq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. ASR"
      ],
      "metadata": {
        "id": "9ARDFNUs-Dfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def videos_to_texts(audio_files):\n",
        "\n",
        "    try:\n",
        "        deepgram = DeepgramClient(f\"{deepgrame_key}\")\n",
        "        results = []\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            with open(audio_file, \"rb\") as file:\n",
        "                response = deepgram.listen.rest.v(\"1\").transcribe_file({\"buffer\": file.read()}, {\"model\": \"nova-2\", \"smart_format\": True})\n",
        "\n",
        "            text = \"\"\n",
        "            for channel in json.loads(response.to_json())['results']['channels']:\n",
        "                text += \" \".join([s['text'] for p in channel['alternatives'][0]['paragraphs']['paragraphs'] for s in p['sentences']]) + \" \"\n",
        "\n",
        "            word_count = len(text.split())\n",
        "\n",
        "            audio_clip = AudioFileClip(audio_file)\n",
        "            duration = audio_clip.duration\n",
        "            audio_clip.close()\n",
        "\n",
        "            results.append((text, duration, word_count))\n",
        "\n",
        "        original_text_segments = [text for text, duration, word_count in results]\n",
        "\n",
        "        return original_text_segments\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception occurred: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "LANDrpUf-Dfr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. LLM"
      ],
      "metadata": {
        "id": "4peW29Le-Dfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LLM_modify_text(original_text_segments):\n",
        "\n",
        "    original_text_dict = {index: value for index, value in enumerate(original_text_segments)}\n",
        "\n",
        "    prompt = (f'''\n",
        "    Please modify the following transcript which comes from a transcript as a whole, with attention to the following requirements:\n",
        "    1. Correct any grammatical mistakes and mispronunciations;\n",
        "    2. Keep the total number of segments unchanged;\n",
        "    3. Ensure that the word count per segment remains approximately the same;\n",
        "    4. Make as few alterations as necessary;\n",
        "    5. The content of the revised segments should exactly resemble that of the original;\n",
        "    6. Format the output as a python dictionary, with double quotes (\") instead of single quotes (') for keys and values.\n",
        "\n",
        "    Transcript:\n",
        "    {original_text_dict}\n",
        "    ''')\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    revised_transcript = response['choices'][0]['message']['content']\n",
        "\n",
        "    parsed_data = json.loads(revised_transcript)\n",
        "    polished_text_segments = list(parsed_data.values())\n",
        "\n",
        "    return polished_text_segments\n"
      ],
      "metadata": {
        "id": "3QE7gpVp-Dfr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Voice Clone"
      ],
      "metadata": {
        "id": "s8XFQqsa-Dfr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hBI13mxj-Dfr"
      },
      "outputs": [],
      "source": [
        "def voice_clone(input_video):\n",
        "\n",
        "    url = \"https://api.play.ht/api/v2/cloned-voices/instant\"\n",
        "\n",
        "    files = { \"sample_file\": (f\"{input_video}\", open(f\"{input_video}\", \"rb\"), \"audio/mpeg\") }\n",
        "    # The audio file selected as the source for the voice clone should have a duration ranging from 2 seconds to 1 hour.\n",
        "    # It can be in any audio format,\n",
        "    # as long as it falls within the size range of 5kb to 50MB.\n",
        "    payload = { \"voice_name\": \"Cloned_Voice\" }\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"AUTHORIZATION\": f\"{playht_key}\",\n",
        "        \"X-USER-ID\": f\"{playht_id}\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, data=payload, files=files, headers=headers)\n",
        "\n",
        "    response_dict = json.loads(response.text)\n",
        "    voiceID = response_dict['id']\n",
        "\n",
        "    return voiceID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. TTS"
      ],
      "metadata": {
        "id": "YHXz2B1N-Dfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_to_url(polish_sentence, speed, voiceID, UserID, UserKey):\n",
        "\n",
        "    if not polish_sentence.strip():\n",
        "        return None\n",
        "\n",
        "    sentence = polish_sentence\n",
        "    url = \"https://api.play.ht/api/v2/tts\"\n",
        "    headers = {\n",
        "        \"accept\": \"text/event-stream\",\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"AUTHORIZATION\": UserKey,\n",
        "        \"X-USER-ID\": UserID\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"text\": sentence,\n",
        "        \"voice\": voiceID,\n",
        "        \"output_format\": \"wav\",\n",
        "        \"voice_engine\": \"PlayHT2.0\",\n",
        "        \"temperature\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"speed\": speed,\n",
        "        \"voice_guidance\": 1,\n",
        "        \"style_guidance\": 1,\n",
        "        \"sample_rate\": 24000\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "    for line in response.text.splitlines():\n",
        "        if line.startswith('data: '):\n",
        "            data = line[len('data: '):]\n",
        "            try:\n",
        "                json_data = json.loads(data)\n",
        "                if json_data.get('stage') == 'complete':\n",
        "                    URL = json_data.get('url')\n",
        "\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return URL"
      ],
      "metadata": {
        "id": "pRF6Q2fi-Dfr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blocks_to_urls(polish_blocks, voiceID, UserID, UserKey, speed):\n",
        "\n",
        "    URLs = []\n",
        "    for block in polish_blocks:\n",
        "        URL = text_to_url(block, speed, voiceID, UserID, UserKey)\n",
        "        URLs.append(URL)\n",
        "\n",
        "    return URLs"
      ],
      "metadata": {
        "id": "iiJVt3Zp-Dfr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Align Video and Audio"
      ],
      "metadata": {
        "id": "Hnt2h2Ty-Dfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_segments(urls, original_audio_durations, silent_segments):\n",
        "    final_audio_fixed = AudioSegment.empty()\n",
        "    audio_durations = []\n",
        "\n",
        "    for key, (start_time, end_time) in silent_segments.items():\n",
        "        if key < 1:\n",
        "            time_diff = abs(end_time - start_time)\n",
        "            if time_diff <= 2:\n",
        "                time_diff = time_diff + 1\n",
        "\n",
        "            silence_duration = time_diff * 1000\n",
        "            silence_audio = AudioSegment.silent(duration=silence_duration)\n",
        "            final_audio_fixed += silence_audio\n",
        "\n",
        "    for index, url in enumerate(urls):\n",
        "\n",
        "        final_audio_fixed += AudioSegment.silent(duration=1000)\n",
        "        if url is None:\n",
        "\n",
        "            silence_duration = original_audio_durations[index] * 1000\n",
        "            silence_audio = AudioSegment.silent(duration=silence_duration)\n",
        "            final_audio_fixed += silence_audio\n",
        "            audio_durations.append(silence_duration / 1000)\n",
        "        else:\n",
        "            response = requests.get(url)\n",
        "            audio = AudioSegment.from_file(BytesIO(response.content), format=\"wav\")\n",
        "\n",
        "            audio_duration = audio.duration_seconds\n",
        "            audio_durations.append(audio_duration)\n",
        "\n",
        "            final_audio_fixed += audio\n",
        "\n",
        "            if index == len(urls) - 1:\n",
        "                final_audio_fixed += AudioSegment.silent(duration=1000)\n",
        "\n",
        "        i = index\n",
        "        if i in silent_segments:\n",
        "            start_time, end_time = silent_segments[i]\n",
        "            time_diff = abs(end_time - start_time)\n",
        "\n",
        "            if time_diff <= 2:\n",
        "              time_diff = time_diff + 1\n",
        "\n",
        "            silence_duration = time_diff * 1000\n",
        "            silence_audio = AudioSegment.silent(duration=silence_duration)\n",
        "            final_audio_fixed += silence_audio\n",
        "\n",
        "    modified_audio = final_audio_fixed\n",
        "    total_duration = modified_audio.duration_seconds\n",
        "\n",
        "    for idx, audio_length in enumerate(audio_durations):\n",
        "        print(f\"Duration of audio {idx + 1} is {audio_length} seconds\")\n",
        "\n",
        "    print(f\"Total duration of the concatenated audio is {total_duration} seconds\")\n",
        "\n",
        "    return modified_audio, total_duration, audio_durations\n"
      ],
      "metadata": {
        "id": "8-Z6cHw1-Dfs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_segments(video_path, original_timestamps, final_audio_durations, slide_pictures, silent_segments):\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    original_video_segments = [video.subclip(original_timestamps[i], int(original_timestamps[i + 1])) for i in range(len(original_timestamps) - 1)]\n",
        "    final_audio_durations = [duration + 1 for duration in final_audio_durations]\n",
        "\n",
        "    polished_video_segments = []\n",
        "    for key, (start_time, end_time) in silent_segments.items():\n",
        "        if key < 1:\n",
        "            time_diff = abs(end_time - start_time)\n",
        "            if time_diff <= 2:\n",
        "                end_time = end_time - 1\n",
        "            silent_video_segment = video.subclip(start_time, end_time).without_audio()\n",
        "            polished_video_segments.append(silent_video_segment)\n",
        "\n",
        "    for i, video_segment in enumerate(original_video_segments):\n",
        "        original_duration = video_segment.duration\n",
        "\n",
        "        if i == len(original_video_segments) - 1:\n",
        "            target_duration = final_audio_durations[i] + 1\n",
        "        else:\n",
        "            target_duration = final_audio_durations[i]\n",
        "\n",
        "        if original_duration > target_duration:\n",
        "            trimmed_segment = video_segment.subclip(0, target_duration).without_audio()\n",
        "            polished_segment = trimmed_segment\n",
        "        else:\n",
        "            slide_image = slide_pictures[i]\n",
        "            slide_image_bgr = cv2.cvtColor(slide_image, cv2.COLOR_RGB2BGR)\n",
        "            slide_clip = ImageClip(slide_image).set_duration(target_duration - original_duration)\n",
        "            slide_clip = slide_clip.resize(height=video_segment.h)\n",
        "            extended_segment = concatenate_videoclips([video_segment, slide_clip])\n",
        "            polished_segment = extended_segment\n",
        "\n",
        "        if i in silent_segments:\n",
        "            start_time, end_time = silent_segments[i]\n",
        "            time_diff = abs(end_time - start_time)\n",
        "\n",
        "            if time_diff <= 2:\n",
        "                start_time = max(0, start_time - 1)  # Ensure start_time doesn't go negative\n",
        "\n",
        "            silent_video_segment = video.subclip(start_time, end_time).without_audio()\n",
        "            extended_silent_segment = concatenate_videoclips([polished_segment, silent_video_segment])\n",
        "        else:\n",
        "            extended_silent_segment = polished_segment\n",
        "\n",
        "        polished_video_segments.append(extended_silent_segment)\n",
        "\n",
        "    return polished_video_segments"
      ],
      "metadata": {
        "id": "ctHxjEpm-Dfs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_final_video(polished_video_segments, final_audio, final_video_path, audioSegmentation):\n",
        "\n",
        "    final_video = concatenate_videoclips(polished_video_segments)\n",
        "    temp_audio_file = f\"{directory_path}/temporary/final_audio.wav\"\n",
        "    final_audio.export(temp_audio_file, format=\"wav\")\n",
        "\n",
        "    final_audio_clip = AudioFileClip(temp_audio_file)\n",
        "    final_video = final_video.set_audio(final_audio_clip)\n",
        "    final_video.write_videofile(final_video_path, codec=\"libx264\", audio_codec=\"aac\", fps=24)\n",
        "\n",
        "    os.remove(temp_audio_file)\n",
        "\n",
        "    print(\"Final video saved successfully!\")"
      ],
      "metadata": {
        "id": "CrbOegmy-Dfs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input information"
      ],
      "metadata": {
        "id": "foYlkGaB-Dfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "playht_key = 'put your api information here'\n",
        "playht_id = 'put your api information here'\n",
        "\n",
        "deepgrame_key = 'put your api information here'\n",
        "openai.api_key = 'put your api information here'\n",
        "directory_path = 'temporal file will be save here when running the code'\n",
        "input_video = 'input video path'\n",
        "output_video = 'output video path'"
      ],
      "metadata": {
        "id": "xs6lKTZB-Dfs"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running code"
      ],
      "metadata": {
        "id": "gmQny9BI-Dfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speed = 0.85\n",
        "slides_detect_threshold = 0.2\n",
        "ssim_threshold = 0.9\n",
        "\n",
        "# 1. Video Frame Recognition\n",
        "original_timestamps, detect_slide_pictures = extract_slide_changes(input_video, slides_detect_threshold)\n",
        "detect_slide_pictures, original_timestamps = delete_repeat_pictures(detect_slide_pictures, original_timestamps)\n",
        "time_group = Time_group(original_timestamps, detect_slide_pictures)\n",
        "ssim_group = SSIM_group(time_group, ssim_threshold)\n",
        "silent_segments, original_text_timestamps, final_slide_pictures = silent_segment(ssim_group)\n",
        "\n",
        "# 2. Audio Segmentation\n",
        "original_audio_segments, original_audio_durations = extract_audio_segments(input_video, original_text_timestamps, directory_path)\n",
        "\n",
        "# 3. ASR\n",
        "original_text_segments = videos_to_texts(original_audio_segments)\n",
        "\n",
        "# 4. LLM\n",
        "polished_text_segments = LLM_modify_text(original_text_segments)\n",
        "\n",
        "# 5. Voice Clone\n",
        "voiceID = voice_clone(input_video)\n",
        "\n",
        "# 6. TTS\n",
        "URLs = blocks_to_urls(polished_text_segments, voiceID, playht_id, playht_key, speed)\n",
        "\n",
        "# 7. Align Video and Audio\n",
        "final_audio, final_total_duration, final_audio_durations = process_audio_segments(URLs, original_audio_durations, silent_segments)\n",
        "polished_video_segments = process_video_segments(input_video, original_text_timestamps, final_audio_durations, final_slide_pictures, silent_segments)\n",
        "save_final_video(polished_video_segments, final_audio, output_video, directory_path)"
      ],
      "metadata": {
        "id": "l2_RCylv-Dfs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}